{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzevedoGabriel/AWS-MQTT-CONNECT/blob/main/C%C3%B3pia_de_Agrupamento_de_textos_com_K_Means_em_Hinos_Nacionais.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este notebook foi adaptado de: https://medium.com/@lucasdesa/clusterização-de-textos-com-k-means-46254fe31bf6 e https://towardsdatascience.com/evaluating-goodness-of-clustering-for-unsupervised-learning-case-ccebcfd1d4f1"
      ],
      "metadata": {
        "id": "DkwYtdjjBJ6r"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T45J8srJDZbr"
      },
      "source": [
        "# Agrupamento de textos com K-Means\n",
        "\n",
        "Neste notebook, usaremos o [algoritmo k-means](https://www.datascience.com/blog/k-means-clustering), um algoritmo simples e popular de __*agrupamento não-supervisionado*__, para agrupar os hinos nacionais de vários países em diferentes grupos.\n",
        "\n",
        "O objetivo do K-means é simples: agrupar pontos de dados semelhantes e descobrir padrões subjacentes. Para atingir esse objetivo, o K-means procura um número fixo definido (k) de centróides em um conjunto de dados. Um centróide refere-se a um cluster, que é uma coleção de pontos de dados agregados devido a certas semelhanças entre si. As **médias/means** no K-means referem-se à média dos dados; isto é, encontrar o centróide. E o algoritmo é dito não supervisionado porque não temos conhecimento prévio sobre os grupos ou classes de nosso conjunto de dados, ou seja, encontraremos os grupos subjacentes em nosso conjunto de dados!\n",
        "\n",
        "Abaixo podemos visualizar o algoritmo. Os centróides verdes correspondem aos pontos de dados mais próximos de cada um e formam clusters, então cada centróide se move para o centro de cada respectivo grupo e combina novamente os pontos de dados mais próximos entre si."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr-3zK-UDZbu"
      },
      "source": [
        "![alt text](https://github.com/lucas-de-sa/national-anthems-clustering/blob/master/Images/kmeans.gif?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yZXoeqKDZbu"
      },
      "source": [
        "**Passos:**\n",
        "\n",
        "__1.__ Explorar nossa coleção de hinos nacionais (corpus) <br>\n",
        "__2.__ Aplicar engenharia de dados no conjunto de dados para obter o melhor desempenho do algoritmo K-means <br>\n",
        "__3.__ Execute o algoritmo várias vezes, cada vez testando com um número diferente de clusters <br>\n",
        "__4.__ Use diferentes métricas para visualizar nossos resultados e encontrar o melhor número de clusters (*ou seja, por que um total de X clusters é melhor do que um total de Y clusters?*) <br>\n",
        "__5.__ Análise de cluster\n",
        "\n",
        "**Métricas utilizadas para determinar o melhor número de K Cluters:**\n",
        "- [Método do cotovelo](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
        "- [Silhouette Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftU5zlICDZbu"
      },
      "source": [
        "## Importando bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1xqJhNoGDoSf",
        "outputId": "68f7928b-3974-4ffa-9228-8280e396e0c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/235.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m235.5/235.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5b2QAihsDZbv",
        "outputId": "44722300-ed8d-439f-ca2c-c68863a626a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Data Structures\n",
        "import numpy  as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import json\n",
        "\n",
        "# Corpus Processing\n",
        "import re\n",
        "import nltk.corpus\n",
        "from unidecode                        import unidecode\n",
        "from nltk.tokenize                    import word_tokenize\n",
        "from nltk                             import SnowballStemmer\n",
        "from sklearn.feature_extraction.text  import TfidfVectorizer\n",
        "from sklearn.preprocessing            import normalize\n",
        "\n",
        "# K-Means\n",
        "from sklearn import cluster\n",
        "\n",
        "# Visualization and Analysis\n",
        "import matplotlib.pyplot  as plt\n",
        "import matplotlib.cm      as cmm\n",
        "import seaborn            as sns\n",
        "from sklearn.metrics                  import silhouette_samples, silhouette_score\n",
        "from wordcloud                        import WordCloud\n",
        "\n",
        "# Map Viz\n",
        "import folium\n",
        "#import branca.colormap as cm\n",
        "from branca.element import Figure\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "MDfNQGhYxnL7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB_hWjv9DZbw"
      },
      "source": [
        "## Carregando o corpus\n",
        "\n",
        "Usaremos pandas para ler o arquivo csv contendo o hino nacional de cada país e seu código de país correspondente. Os hinos foram extraídos da wikipedia e muitos deles contém palavras que usam caracteres não UTF-8 (geralmente nomes de lugares e tal), então vamos ler o arquivo com a codificação _latin1_.\n",
        "\n",
        "Em seguida, extrairemos a coluna __Anthem__ em uma lista de textos para nosso corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "aq6ldTJeDZbw",
        "outputId": "a2a53661-fb3b-4cc4-aa69-815e16838898"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       author  \\\n",
              "0            Cecília Meireles   \n",
              "1             Fernando Pessoa   \n",
              "2            Marina Colasanti   \n",
              "3  Carlos Drummond de Andrade   \n",
              "4          Eugénio de Andrade   \n",
              "5          Vinicius de Moraes   \n",
              "\n",
              "                                             content  \n",
              "0  Retrato\\nEu não tinha este rosto de hoje,\\nAss...  \n",
              "1  Para ser grande, sê inteiro: nada\\nPara ser gr...  \n",
              "2  Eu sei, mas não devia\\nEu sei que a gente se a...  \n",
              "3  Quadrilha\\nJoão amava Teresa que amava Raimund...  \n",
              "4  É urgente o amor\\nÉ urgente o amor.\\nÉ urgente...  \n",
              "5  Procura-se um amigo\\nNão precisa ser homem, ba...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-31c651e1-0cbc-4e3d-a474-fee200421340\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cecília Meireles</td>\n",
              "      <td>Retrato\\nEu não tinha este rosto de hoje,\\nAss...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Fernando Pessoa</td>\n",
              "      <td>Para ser grande, sê inteiro: nada\\nPara ser gr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Marina Colasanti</td>\n",
              "      <td>Eu sei, mas não devia\\nEu sei que a gente se a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Carlos Drummond de Andrade</td>\n",
              "      <td>Quadrilha\\nJoão amava Teresa que amava Raimund...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Eugénio de Andrade</td>\n",
              "      <td>É urgente o amor\\nÉ urgente o amor.\\nÉ urgente...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Vinicius de Moraes</td>\n",
              "      <td>Procura-se um amigo\\nNão precisa ser homem, ba...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-31c651e1-0cbc-4e3d-a474-fee200421340')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-31c651e1-0cbc-4e3d-a474-fee200421340 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-31c651e1-0cbc-4e3d-a474-fee200421340');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3c53faab-397a-4453-8999-4c6ae6a31fd8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3c53faab-397a-4453-8999-4c6ae6a31fd8')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3c53faab-397a-4453-8999-4c6ae6a31fd8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data = pd.read_csv('https://gist.githubusercontent.com/issilva5/b4a1c6dc5989ad83663ae02929f2894c/raw/28f6f957f0e9e9164027992dea52860e001ca80b/poemas.csv', encoding='utf-8')\n",
        "data.columns = map(str.lower, data.columns)\n",
        "\n",
        "\n",
        "data.head(6)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def getAuthorList(data):\n",
        "     authors = data['author'].to_list()\n",
        "     authors = [author.lower().split(' ') for author in authors]\n",
        "     authors = sum(authors, [])\n",
        "     authors = list(set(authors))\n",
        "     return authors\n",
        "\n",
        "authors = getAuthorList(data)"
      ],
      "metadata": {
        "id": "Bx5bAPMH2jmy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bqvvgUqBDZbw"
      },
      "outputs": [],
      "source": [
        "corpus = data['content'].tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpq6Qlk4DZbx"
      },
      "source": [
        "## Processando o corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Stop Words and Stemming\n",
        "Faremos uma rotina de engenharia de dados com nosso dataset de hinos para posteriormente fazermos um bom modelo estatístico. Para isso, removeremos todas as palavras que não contribuam para o significado semântico do texto (palavras que não estão dentro do alfabeto inglês) e manteremos todas as palavras restantes no formato mais simples possível, para que possamos aplicar uma função que dê pesos a cada palavra sem gerar nenhum viés ou outliers. Para isso existem várias técnicas para limpar nosso corpus, entre elas vamos remover as palavras mais comuns ([stop words](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/)) e aplicar [stemming](https://www.researchgate.net/figure/Stemming-process-Algorithms-of-stemming-methods-are-divided-into-three-parts-mixed_fig2_324685008), uma técnica que reduz uma palavra a é raiz.\n",
        "\n",
        "Os métodos que aplicam a remoção de stemming e stop words estão listados abaixo. Também definiremos um método que remove todas as palavras com menos de 2 letras ou mais de 21 letras para limpar ainda mais nosso corpus."
      ],
      "metadata": {
        "id": "AFlnfKl6xPfg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VY8WFr9DZbx"
      },
      "outputs": [],
      "source": [
        "# removes a list of words (ie. stopwords) from a tokenized list.\n",
        "def removeWords(listOfTokens, listOfWords):\n",
        "    return [token for token in listOfTokens if token not in listOfWords]\n",
        "\n",
        "# applies stemming to a list of tokenized words\n",
        "def applyStemming(listOfTokens, stemmer):\n",
        "    return [stemmer.stem(token) for token in listOfTokens]\n",
        "\n",
        "# removes any words composed of less than 2 or more than 21 letters\n",
        "def twoLetters(listOfTokens):\n",
        "    twoLetterWord = []\n",
        "    for token in listOfTokens:\n",
        "        if len(token) <= 2 or len(token) >= 21:\n",
        "            twoLetterWord.append(token)\n",
        "    return twoLetterWord"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fSoyNt_DZbx"
      },
      "source": [
        "### 2. A função principal de processamento\n",
        "\n",
        "Uma seção atrás, na exploração de nosso conjunto de dados, notamos algumas palavras contendo caracteres estranhos que deveriam ser removidos. Ao usar o RegEx, nossa principal função de processamento removerá símbolos ASCII desconhecidos, caracteres especiais, números, e-mails, URLs, etc. Ele também usa as funções auxiliares definidas acima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QieUtjhuDZbx"
      },
      "outputs": [],
      "source": [
        "def processCorpus(corpus, language):\n",
        "    stopwords = nltk.corpus.stopwords.words(language)\n",
        "    param_stemmer = SnowballStemmer(language)\n",
        "    countries_list = [line.decode('utf-8').rstrip('\\n') for line in urllib.request.urlopen('https://gist.githubusercontent.com/issilva5/b4a1c6dc5989ad83663ae02929f2894c/raw/28f6f957f0e9e9164027992dea52860e001ca80b/poemas.csv')] # Load .txt file line by line\n",
        "    nationalities_list = [line.decode('utf-8').rstrip('\\n') for line in urllib.request.urlopen('https://gist.githubusercontent.com/issilva5/b4a1c6dc5989ad83663ae02929f2894c/raw/28f6f957f0e9e9164027992dea52860e001ca80b/poemas.csv')] # Load .txt file line by line\n",
        "    other_words = [line.decode('utf-8').rstrip('\\n') for line in urllib.request.urlopen('https://gist.githubusercontent.com/issilva5/b4a1c6dc5989ad83663ae02929f2894c/raw/28f6f957f0e9e9164027992dea52860e001ca80b/poemas.csv')] # Load .txt file line by line\n",
        "\n",
        "    for document in corpus:\n",
        "        index = corpus.index(document)\n",
        "        corpus[index] = corpus[index].replace(u'\\ufffd', '8')   # Replaces the ASCII '�' symbol with '8'\n",
        "        corpus[index] = corpus[index].replace(',', '')          # Removes commas\n",
        "        corpus[index] = corpus[index].rstrip('\\n')              # Removes line breaks\n",
        "        corpus[index] = corpus[index].casefold()                # Makes all letters lowercase\n",
        "\n",
        "        corpus[index] = re.sub('\\W_',' ', corpus[index])        # removes specials characters and leaves only words\n",
        "        corpus[index] = re.sub(\"\\S*\\d\\S*\",\" \", corpus[index])   # removes numbers and words concatenated with numbers IE h4ck3r. Removes road names such as BR-381.\n",
        "        corpus[index] = re.sub(\"\\S*@\\S*\\s?\",\" \", corpus[index]) # removes emails and mentions (words with @)\n",
        "        corpus[index] = re.sub(r'http\\S+', '', corpus[index])   # removes URLs with http\n",
        "        corpus[index] = re.sub(r'www\\S+', '', corpus[index])    # removes URLs with www\n",
        "\n",
        "        listOfTokens = word_tokenize(corpus[index])\n",
        "        twoLetterWord = twoLetters(listOfTokens)\n",
        "\n",
        "        listOfTokens = removeWords(listOfTokens, stopwords)\n",
        "        listOfTokens = removeWords(listOfTokens, twoLetterWord)\n",
        "        listOfTokens = removeWords(listOfTokens, countries_list)\n",
        "        listOfTokens = removeWords(listOfTokens, nationalities_list)\n",
        "        listOfTokens = removeWords(listOfTokens, other_words)\n",
        "\n",
        "        listOfTokens = applyStemming(listOfTokens, param_stemmer)\n",
        "        listOfTokens = removeWords(listOfTokens, other_words)\n",
        "\n",
        "        corpus[index]   = \" \".join(listOfTokens)\n",
        "        corpus[index] = unidecode(corpus[index])\n",
        "\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "EShQ85MzDZby",
        "outputId": "71e7f533-ee1e-497e-e27b-febcb07bba3a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-c4b77510ed8b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'portuguese'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-61-8d41ecd3f37e>\u001b[0m in \u001b[0;36mprocessCorpus\u001b[0;34m(corpus, language)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mlistOfTokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapplyStemming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistOfTokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_stemmer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mlistOfTokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremoveWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistOfTokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistOfTokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-3c6e3f06b591>\u001b[0m in \u001b[0;36mremoveWords\u001b[0;34m(listOfTokens, listOfWords)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# removes a list of words (ie. stopwords) from a tokenized list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremoveWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistOfTokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlistOfWords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistOfTokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistOfWords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# applies stemming to a list of tokenized words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-3c6e3f06b591>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# removes a list of words (ie. stopwords) from a tokenized list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremoveWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistOfTokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlistOfWords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistOfTokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistOfWords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# applies stemming to a list of tokenized words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "language = 'portuguese'\n",
        "corpus = processCorpus(corpus, language)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY1vpNmyDZby"
      },
      "source": [
        "### Aplicando a vetorização com TF-IDF\n",
        "\n",
        "Agora vamos aplicar a função [TF-IDF](https://jmotif.github.io/sax-vsm_site/morea/algorithm/TFIDF.html), abreviação de frequência do documento inverso da frequência do termo, que é uma estatística numérica que se destina para refletir a importância de uma palavra para um documento em um corpus, atribuindo a cada palavra em um documento uma pontuação que varia de 0 a 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Sq9v5BYDZby"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "tf_idf = pd.DataFrame(data = X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "final_df = tf_idf\n",
        "\n",
        "print(\"{} rows\".format(final_df.shape[0]))\n",
        "final_df.T.nlargest(5, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVdY-njXDZby"
      },
      "outputs": [],
      "source": [
        "# first 5 words with highest weight on document 0:\n",
        "final_df.T.nlargest(5, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzMgZQZZDZby"
      },
      "source": [
        "## Aplicando o K-Means\n",
        "\n",
        "Função que executa o algoritmo K-Means *max_k* vezes e retorna um dicionário de cada k resultante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkNAVTTtDZby"
      },
      "outputs": [],
      "source": [
        "def run_KMeans(max_k, data):\n",
        "    max_k += 1\n",
        "    kmeans_results = dict()\n",
        "    for k in range(2 , max_k):\n",
        "        kmeans = cluster.KMeans(n_clusters = k\n",
        "                               , init = 'k-means++'\n",
        "                               , n_init = 1\n",
        "                               , tol = 0.0001\n",
        "                               , random_state = 1\n",
        "                               , algorithm = 'full')\n",
        "\n",
        "        kmeans_results.update( {k : kmeans.fit(data)} )\n",
        "\n",
        "    return kmeans_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running Kmeans\n",
        "k = 8\n",
        "kmeans_results = run_KMeans(k, final_df)"
      ],
      "metadata": {
        "id": "94iOUbuFyISV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encontrando o melhor resultado"
      ],
      "metadata": {
        "id": "JMFKC8jux5Qm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Método do joelho\n",
        "\n",
        "Calcularemos o método do joelho usando a soma do quadrado das distâncias das ammostras a seus respectivos centros"
      ],
      "metadata": {
        "id": "pCe_Zmu09AFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum_of_squared_distances = []\n",
        "n = range(2, k)\n",
        "for i in n:\n",
        "    sum_of_squared_distances.append(kmeans_results[i].inertia_)\n",
        "\n",
        "plt.plot(n, sum_of_squared_distances, 'bx-')\n",
        "plt.xlabel(\"Número de clusters\")\n",
        "plt.ylabel(\"Soma dos quadrados\")\n",
        "plt.title('Método do cotovelo para encontrar o k ótimo')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vfZGC8zc8_mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odkIn_vSDZby"
      },
      "source": [
        "#### Silhouette Score\n",
        "\n",
        "O Silhouette Score é uma medida de quão semelhante um objeto é ao seu próprio cluster (coesão) em comparação com outros clusters (separação)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8q9iRT14DZby"
      },
      "outputs": [],
      "source": [
        "def printAvg(avg_dict):\n",
        "    for avg in sorted(avg_dict.keys(), reverse=True):\n",
        "        print(\"Avg: {}\\tK:{}\".format(avg.round(4), avg_dict[avg]))\n",
        "\n",
        "def plotSilhouette(df, n_clusters, splotn, plot_idx, kmeans_labels, silhouette_avg):\n",
        "    ax1 = plt.subplot(splotn, splotn, plot_idx)\n",
        "    ax1.set_xlim([-0.2, 1])\n",
        "    ax1.set_ylim([0, len(df) + (n_clusters + 1) * 10])\n",
        "\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\") # The vertical line for average silhouette score of all the values\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "    plt.title((\"K = %d, SS = %.4f\" % (n_clusters, silhouette_avg)), fontsize=10, fontweight='bold')\n",
        "\n",
        "    y_lower = 10\n",
        "    sample_silhouette_values = silhouette_samples(df, kmeans_labels) # Compute the silhouette scores for each sample\n",
        "\n",
        "    for i in range(n_clusters):\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[kmeans_labels == i]\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cmm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i)) # Label the silhouette plots with their cluster numbers at the middle\n",
        "        y_lower = y_upper + 10  # Compute the new y_lower for next plot. 10 for the 0 samples\n",
        "\n",
        "\n",
        "def silhouette(kmeans_dict, df, plot=False):\n",
        "    df = df.to_numpy()\n",
        "    avg_dict = dict()\n",
        "    subplots_num = int(np.ceil(np.sqrt(k)))\n",
        "    plot_idx = 0\n",
        "\n",
        "    if plot:\n",
        "        fig = plt.figure(figsize=(15,15))\n",
        "\n",
        "    for n_clusters, kmeans in kmeans_dict.items():\n",
        "        plot_idx += 1\n",
        "        kmeans_labels = kmeans.predict(df)\n",
        "        silhouette_avg = silhouette_score(df, kmeans_labels) # Average Score for all Samples\n",
        "        avg_dict.update( {n_clusters : silhouette_avg} )\n",
        "\n",
        "        if(plot): plotSilhouette(df, n_clusters, subplots_num, plot_idx, kmeans_labels, silhouette_avg)\n",
        "\n",
        "    if plot:\n",
        "        plt.show()\n",
        "\n",
        "    return avg_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7R4qF5wDZbz"
      },
      "outputs": [],
      "source": [
        "# Plotting Silhouette Analysis\n",
        "ss_dict = silhouette(kmeans_results, final_df, plot=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_silhouette = pd.DataFrame({'n_clusters':ss_dict.keys(),'silhouette_score':ss_dict.values()})\n",
        "ax = sns.lineplot(data=df_silhouette, x=\"n_clusters\", y=\"silhouette_score\")\n",
        "ax.set(xlabel=\"Número de clusters\", ylabel=\"Score\", title=\"Média do Silhouette Score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wj-qmToP65z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtyd7qCODZbz"
      },
      "source": [
        "## Análise nos clusters\n",
        "\n",
        "Agora podemos escolher o melhor número de K e dar uma olhada mais profunda em cada cluster. Olhando para os gráficos acima, temos algumas pistas de que quando K = 6 é quando os clusters são melhor definidos."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Histograma das palavras\n",
        "\n",
        "Então, primeiro usaremos um histograma simples para observar as palavras mais dominantes em cada grupo:"
      ],
      "metadata": {
        "id": "iGjiYy5TAOi-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLbPlkAGDZbz"
      },
      "outputs": [],
      "source": [
        "def get_top_features_cluster(tf_idf_array, prediction, n_feats):\n",
        "    labels = np.unique(prediction)\n",
        "    dfs = []\n",
        "    for label in labels:\n",
        "        id_temp = np.where(prediction==label) # indices for each cluster\n",
        "        x_means = np.mean(tf_idf_array[id_temp], axis = 0) # returns average score across cluster\n",
        "        sorted_means = np.argsort(x_means)[::-1][:n_feats] # indices with top 20 scores\n",
        "        features = vectorizer.get_feature_names_out()\n",
        "        best_features = [(features[i], x_means[i]) for i in sorted_means]\n",
        "        df = pd.DataFrame(best_features, columns = ['features', 'score'])\n",
        "        dfs.append(df)\n",
        "    return dfs\n",
        "\n",
        "def plotWords(dfs, n_feats):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    for i in range(0, len(dfs)):\n",
        "        plt.title((\"Palavras mais comuns Cluster {}\".format(i)), fontsize=10, fontweight='bold')\n",
        "        sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[i][:n_feats])\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x-mU-JWDZbz"
      },
      "outputs": [],
      "source": [
        "best_result = 6\n",
        "kmeans = kmeans_results.get(best_result)\n",
        "\n",
        "final_df_array = final_df.to_numpy()\n",
        "prediction = kmeans.predict(final_df)\n",
        "n_feats = 20\n",
        "dfs = get_top_features_cluster(final_df_array, prediction, n_feats)\n",
        "plotWords(dfs, 13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w5ntUhADZbz"
      },
      "source": [
        "#### Wordcloud\n",
        "\n",
        "Agora que podemos olhar os gráficos acima e ver as palavras mais bem pontuadas em cada cluster, também é interessante deixá-lo mais bonito fazendo um mapa de palavras de cada cluster!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8uWfCYvDZbz"
      },
      "outputs": [],
      "source": [
        "# Transforms a centroids dataframe into a dictionary to be used on a WordCloud.\n",
        "def centroidsDict(centroids, index):\n",
        "    a = centroids.T[index].sort_values(ascending = False).reset_index().values\n",
        "    centroid_dict = dict()\n",
        "\n",
        "    for i in range(0, len(a)):\n",
        "        centroid_dict.update( {a[i,0] : a[i,1]} )\n",
        "\n",
        "    return centroid_dict\n",
        "\n",
        "def generateWordClouds(centroids):\n",
        "    wordcloud = WordCloud(max_font_size=100, background_color = 'white')\n",
        "    for i in range(0, len(centroids)):\n",
        "        centroid_dict = centroidsDict(centroids, i)\n",
        "        wordcloud.generate_from_frequencies(centroid_dict)\n",
        "\n",
        "        plt.figure()\n",
        "        plt.title('Cluster {}'.format(i))\n",
        "        plt.imshow(wordcloud)\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVdRB5_bDZbz"
      },
      "outputs": [],
      "source": [
        "centroids = pd.DataFrame(kmeans.cluster_centers_)\n",
        "centroids.columns = final_df.columns\n",
        "generateWordClouds(centroids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMzjuxXnDZbz"
      },
      "source": [
        "### Visualizando no map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_N9NeiXHDZbz"
      },
      "outputs": [],
      "source": [
        "# # Assigning the cluster labels to each country\n",
        "# labels = kmeans.labels_\n",
        "# data['label'] = labels\n",
        "# data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSg9T0B-DZbz"
      },
      "source": [
        "Agora que temos nosso agrupamento final, seria muito legal visualizá-lo em um mapa interativo. Para fazer isso, usaremos a incrível biblioteca Folium para ver nosso mapa interativo!\n",
        "\n",
        "Carregaremos um arquivo geojson de polígonos e códigos de país com geopandas e o mesclaremos com o dataframe rotulado da célula acima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAc9aGP6DZbz"
      },
      "outputs": [],
      "source": [
        "# # Map Viz\n",
        "# import json\n",
        "# import geopandas as gpd\n",
        "\n",
        "# # Loading countries polygons\n",
        "# geo_path = 'https://raw.githubusercontent.com/lucas-de-sa/national-anthems-clustering/master/datasets/world-countries.json'\n",
        "# country_geo = json.load(urllib.request.urlopen(geo_path))\n",
        "# gpf = gpd.read_file(geo_path)\n",
        "\n",
        "# # Merging on the alpha-3 country codes\n",
        "# merge = pd.merge(gpf, data, left_on='id', right_on='alpha-3')\n",
        "# data_to_plot = merge[[\"id\", \"name\", \"label\", \"geometry\"]]\n",
        "\n",
        "# data_to_plot.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17tJuUmCDZb0"
      },
      "outputs": [],
      "source": [
        "# import branca.colormap as cm\n",
        "\n",
        "# # Creating a discrete color map\n",
        "# values = data_to_plot[['label']].to_numpy()\n",
        "# color_step = cm.StepColormap(['r', 'y','g','b', 'purple', 'm'], vmin=values.min(), vmax=values.max(), caption='step')\n",
        "\n",
        "# color_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cToqhkTDZb0"
      },
      "outputs": [],
      "source": [
        "# import folium\n",
        "# from branca.element import Figure\n",
        "\n",
        "# def make_geojson_choropleth(display, data, colors):\n",
        "#     '''creates geojson choropleth map using a colormap, with tooltip for country names and groups'''\n",
        "#     group_dict = data.set_index('id')['label'] # Dictionary of Countries IDs and Clusters\n",
        "#     tooltip = folium.features.GeoJsonTooltip([\"name\", \"label\"], aliases=display, labels=True)\n",
        "#     return folium.GeoJson(data[[\"id\", \"name\",\"label\",\"geometry\"]],\n",
        "#                           style_function = lambda feature: {\n",
        "#                                'fillColor': colors(group_dict[feature['properties']['id']]),\n",
        "#                                #'fillColor': test(feature),\n",
        "#                                'color':'black',\n",
        "#                                'weight':0.5\n",
        "#                                },\n",
        "#                           highlight_function = lambda x: {'weight':2, 'color':'black'},\n",
        "#                           smooth_factor=2.0,\n",
        "#                           tooltip = tooltip)\n",
        "\n",
        "# # Makes map appear inline on notebook\n",
        "# def display(m, width, height):\n",
        "#     \"\"\"Takes a folium instance and embed HTML.\"\"\"\n",
        "#     fig = Figure(width=width, height=height)\n",
        "#     fig.add_child(m)\n",
        "#     #return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq14RJElDZb0"
      },
      "outputs": [],
      "source": [
        "# # Initializing our Folium Map\n",
        "# m = folium.Map(location=[43.5775, -10.106111], zoom_start=2.3, tiles='cartodbpositron')\n",
        "\n",
        "# # Making a choropleth map with geojson\n",
        "# geojson_choropleth = make_geojson_choropleth([\"Country:\", \"Group:\"], data_to_plot, color_step)\n",
        "# geojson_choropleth.add_to(m)\n",
        "\n",
        "# width, height = 1300, 675\n",
        "# display(m, width, height)\n",
        "# m"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretando seus resultados"
      ],
      "metadata": {
        "id": "cuEMIadtBeUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após realizar as visualizações é importante tentar dar uma semântica a cada grupo, por exemplo, podemos ver que o grupo 1 tem um conjunto de palavras mais ligadas a religião, enquanto que o conjunto 3 tem palavras mais ligadas a liberdade."
      ],
      "metadata": {
        "id": "VBCqMvkqBoe5"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dB_hWjv9DZbw",
        "zpq6Qlk4DZbx"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}